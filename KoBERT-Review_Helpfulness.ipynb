{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bd28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoModel, AutoTokenizer, TFGPT2Model, Trainer, TrainingArguments\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, mean_squared_error, roc_auc_score, r2_score, roc_curve, auc, mean_absolute_error, confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (Embedding, Conv1D, LSTM, Dense, Conv2D, GlobalMaxPooling2D, GlobalMaxPooling1D, Reshape)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gensim.models import Word2Vec\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ[\"HF_HOME\"] = \"Nampromotion/KoGPT2-Review_Helpfulness\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d02e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kobert():\n",
    "    # 데이터 로드\n",
    "    train_data = pd.read_csv('/home/olga/NSJ/전처리/train.csv')\n",
    "    test_data = pd.read_csv('/home/olga/NSJ/전처리/test.csv')\n",
    "    \n",
    "    X_train = train_data['review_text']\n",
    "    y_train = train_data['review_usefulness']\n",
    "    X_test = test_data['review_text']\n",
    "    y_test = test_data['review_usefulness']\n",
    "\n",
    "    # 모델을 텐서플로우 버전으로 변환\n",
    "    model_pt = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "    model_pt.save_pretrained(\"./kobert_from_pt\", saved_model=True)\n",
    "    model = TFBertForSequenceClassification.from_pretrained(\"./kobert_from_pt\", from_pt=True, num_labels=2)\n",
    "\n",
    "    # KoBERT 토크나이저 초기화\n",
    "    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "    # 데이터 토크나이징\n",
    "    train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "    test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # 얼리스타핑\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    # 훈련 데이터셋 준비\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(train_encodings),\n",
    "        y_train\n",
    "    )).shuffle(1000).batch(32)\n",
    "\n",
    "    # 검증 데이터셋 준비\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(test_encodings),\n",
    "        y_test\n",
    "    )).batch(32)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(train_dataset, epochs=100, validation_data=test_dataset, callbacks=[early_stopping])\n",
    "\n",
    "    # 예측 및 성능 지표 계산\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred_class = np.argmax(y_pred.logits, axis=1)\n",
    "    y_pred_proba = np.max(tf.nn.softmax(y_pred.logits, axis=1), axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred_class)\n",
    "    f1 = f1_score(y_test, y_pred_class)\n",
    "    precision = precision_score(y_test, y_pred_class)\n",
    "    recall = recall_score(y_test, y_pred_class)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    mae = mean_absolute_error(y_test, y_pred_class)\n",
    "    mse = mean_squared_error(y_test, y_pred_class)\n",
    "    rmse = mean_squared_error(y_test, y_pred_class, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred_class)\n",
    "\n",
    "    # 결과 출력\n",
    "    print('Accuracy:', acc)\n",
    "    print('F1 Score:', f1)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('ROC AUC:', roc_auc)\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    print('Root Mean Squared Error:', rmse)\n",
    "    print('R2 Score:', r2)\n",
    "\n",
    "    # 학습 과정에서의 loss 및 accuracy 시각화\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Loss 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Evolution')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC 커브 그리기\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_class)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return y_test, y_pred.logits\n",
    "\n",
    "# train_kobert 함수 실행 및 결과 받기\n",
    "#y_test_kobert, y_pred_kobert = train_kobert()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
